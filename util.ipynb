{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for compatibility between Python 2&3\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from six.moves import xrange\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import groupby\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import jsonlines\n",
    "from nltk.tokenize import word_tokenize\n",
    "import logging\n",
    "from gensim.models import KeyedVectors\n",
    "import itertools\n",
    "from collections import namedtuple\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.proxy import Proxy, ProxyType\n",
    "from selenium.webdriver.firefox.firefox_binary import FirefoxBinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_driver(browser=\"chrome\", headless=False, extensions=None, proxy=None, user_agent=None):\n",
    "    if browser == \"chrome\":\n",
    "        options = ChromeOptions()\n",
    "        if headless:\n",
    "            options.add_argument(\"-headless\")\n",
    "        if extensions is not None:\n",
    "            if not isinstance(extensions, list):\n",
    "                extensions = [extensions]\n",
    "            for ext in extensions:\n",
    "                options.add_extension(ext)\n",
    "        driver = webdriver.Chrome(chrome_options=options)\n",
    "    elif browser == \"firefox\":\n",
    "        profile = webdriver.FirefoxProfile()\n",
    "        if headless:\n",
    "            options = FirefoxOptions()\n",
    "            options.add_argument(\"--headless\")\n",
    "        else:\n",
    "            options = None\n",
    "        if proxy is not None:\n",
    "            ip = proxy['ip']\n",
    "            port = int(proxy['port'])\n",
    "            profile.set_preference(\"network.proxy.type\", 1)\n",
    "            profile.set_preference(\"network.proxy.http\", ip)\n",
    "            profile.set_preference(\"network.proxy.http_port\", port)\n",
    "            profile.set_preference(\"network.proxy.ssl\", ip)\n",
    "            profile.set_preference(\"network.proxy.ssl_port\", port)\n",
    "        if user_agent is not None:\n",
    "            profile.set_preference(\"general.useragent.override\", user_agent)\n",
    "        \n",
    "        profile.update_preferences()\n",
    "        \n",
    "        driver = webdriver.Firefox(\n",
    "            firefox_profile=profile, \n",
    "            firefox_options=options,\n",
    "            executable_path='./geckodriver',\n",
    "            proxy=proxy)\n",
    "    elif browser == \"safari\":\n",
    "        driver = webdriver.Safari()\n",
    "    else:\n",
    "        print(f\"Error: unknown browser {browser}\")\n",
    "        raise\n",
    "        \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_lst(lst, delimiter, keep_delimiter=True):\n",
    "    \"\"\"\n",
    "    Split list into sublists based on a delimiter\n",
    "    \"\"\"\n",
    "    if keep_delimiter:\n",
    "        append = [delimiter]\n",
    "    else:\n",
    "        append = []\n",
    "    sublists = [list(y) + append\n",
    "                for x, y \n",
    "                in itertools.groupby(lst, lambda z: z == delimiter) \n",
    "                if not x]\n",
    "    return sublists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_lst(lst, delimiter):\n",
    "    \"\"\"\n",
    "    Split list into sublists based on a delimiter\n",
    "    Last modified: 12/4/17\n",
    "    \"\"\"\n",
    "    sublists = [list(y) + [delimiter]\n",
    "                for x, y \n",
    "                in groupby(lst, lambda z: z == delimiter) \n",
    "                if not x]\n",
    "    return sublists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dict(lst1, lst2=[]):\n",
    "    \"\"\"\n",
    "    Build dictionary based on one or two lists.\n",
    "    Args:\n",
    "        lst1\n",
    "        lst2\n",
    "    Returns:\n",
    "        two dictionaries, the second one is the reverse of the first\n",
    "    \"\"\"\n",
    "    if lst2 == []:\n",
    "        key2val = {i: val for (i, val) in enumerate(lst1)}\n",
    "        val2key = {val: i for (val, i) in enumerate(lst1)}\n",
    "    else:\n",
    "        assert len(lst1) == len(lst2), (\n",
    "            \"Error in building dictionary: \"\n",
    "            \"the two lists do not have the same length\")\n",
    "        key2val = {key: val for (key, val) in zip(lst1, lst2)}\n",
    "        val2key = {val: key for (key, val) in zip(lst1, lst2)}\n",
    "        \n",
    "    return (key2val, val2key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_index2token(lst, reverse=False):\n",
    "    if reverse:\n",
    "        dictionary = {val: i for (i, val) in enumerate(lst)}\n",
    "    else:\n",
    "        dictionary = {i: val for (i, val) in enumerate(lst)}\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def have_duplicates(lst):\n",
    "    \"\"\"\n",
    "    Note: Each element in lst needs to be hashable! (i.e., list of lists won't work)\n",
    "    \"\"\"\n",
    "    return len(set(lst)) < len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exists(path):\n",
    "    fp = Path(path)\n",
    "    return fp.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_word2vec_model():\n",
    "    \"\"\"\n",
    "    Load word embedding model\n",
    "    \"\"\"\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s : %(levelname)s : %(message)s', \n",
    "        level=logging.INFO)\n",
    "    model_path = '/playpen/home/tongn/GoogleNews-vectors-negative300.bin'\n",
    "    model = KeyedVectors.load_word2vec_format(fname=model_path, binary=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad(input_seqs, sequence_lengths, pad_token=0, pad_len=None):\n",
    "    \"\"\"\n",
    "    Pad a batch to max_sequence_length along the second dimension\n",
    "    Args:\n",
    "        • input_seqs: a list of sequences\n",
    "        • sequence_lengths: a list of sequence length\n",
    "        • pad_token: token used for padding\n",
    "        • pad_len: maximum lengths to be padded to\n",
    "    Returns:\n",
    "        • padded\n",
    "    \"\"\"\n",
    "    if pad_len:\n",
    "        max_length = pad_len\n",
    "    else:\n",
    "        max_length = max(sequence_lengths)\n",
    "    padded = [input_seq + [pad_token] * (max_length - sequence_length) \n",
    "              for (input_seq, sequence_length)\n",
    "              in zip(input_seqs, sequence_lengths)]\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unzip_lst(lst):\n",
    "    \"\"\"\n",
    "    unzip a list of tuples/lists to multiple lists\n",
    "    \"\"\"\n",
    "    unzipped = list(zip(*lst))\n",
    "    unzipped_lsts = [list(tp) for tp in unzipped]\n",
    "    return unzipped_lsts\n",
    "\n",
    "def zip_lsts(lsts):\n",
    "    \"\"\"\n",
    "    zip a list of lists\n",
    "    \"\"\"\n",
    "    lengths = [len(lst) for lst in lsts]\n",
    "    assert len(list(set(lengths))) == 1 # assert that the lsts have the same lengths\n",
    "    zipped_lst = [list(tp) for tp in list(zip(*lsts))]\n",
    "    return zipped_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as fp:\n",
    "        lst = pickle.load(fp)\n",
    "    print(\"Done loading %s.\" % filename)\n",
    "    return(lst)\n",
    "\n",
    "def load_pickles(filenames):\n",
    "    lsts = []\n",
    "    for filename in filenames:\n",
    "        lsts.append(load_pickle(filename))\n",
    "    return lsts\n",
    "\n",
    "def dump_pickle(filename, lst):\n",
    "    with open(filename, \"wb\") as fp:\n",
    "        pickle.dump(lst, fp)\n",
    "        print(\"Done dumping %s.\" % filename)\n",
    "\n",
    "def dump_pickles(filenames, lsts):\n",
    "    for (filename, lst) in zip(filenames, lsts):\n",
    "        dump_pickle(filename, lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_lines(filename):\n",
    "    \"\"\"\n",
    "    Load a file line by line into a list\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as fp:\n",
    "        lines = fp.readlines()\n",
    "    print(\"Done reading file\", filename)\n",
    "    \n",
    "    return [line.strip() for line in lines]\n",
    "\n",
    "def write_lines(filename, lines, mode='w'):\n",
    "    \"\"\"\n",
    "    Write a list to a file line by line \n",
    "    \"\"\"\n",
    "    with open(filename, mode, encoding=\"utf-8\") as fp:\n",
    "        for line in lines:\n",
    "            print(line, file=fp)\n",
    "    action = 'writing' if mode == 'w' else 'appending'\n",
    "    print(f\"Done {action} to file {filename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def last_occurance_index(string, char):\n",
    "    return string.rfind(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exists(path):\n",
    "    fp = Path(path)\n",
    "    return fp.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_jsonl(path):\n",
    "    data = []\n",
    "    with jsonlines.open(path) as reader:\n",
    "        for obj in reader:\n",
    "            data.append(obj)\n",
    "    print(\"Done reading\", path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    return (word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepend(sents, token_index):\n",
    "    assert [] not in sents # verify that there is no empty list in \"sents\"\n",
    "    assert isinstance(sents[0], list)\n",
    "    prepended = [[token_index] + sent for sent in sents]\n",
    "    return prepended \n",
    "\n",
    "def append(sents, token_index):\n",
    "    assert [] not in sents\n",
    "    assert isinstance(sents[0], list)\n",
    "    appended = [sent + [token_index] for sent in sents]\n",
    "    return appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode2string(index2token, indices, end_token=\"END_TOKEN\", remove_END_TOKEN=False):\n",
    "    \"\"\"\n",
    "    Decode a list of indices to string.\n",
    "    Args:\n",
    "        index2token: a dictionary that maps indices to tokens\n",
    "        indices: a list of indices that correspond to tokens\n",
    "        remove_END_TOKEN: boolean indicating whether to remove the \"END_TOKEN\" (optional)\n",
    "    Returns:\n",
    "        the decoded string\n",
    "    \"\"\"\n",
    "    decoded = [index2token[index] for index in indices]\n",
    "    while True:\n",
    "        if remove_END_TOKEN == True and decoded != []:\n",
    "            if decoded[-1] == end_token:\n",
    "                del decoded[-1]\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    return (' ').join(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_lst(lst, num_grouped):\n",
    "    num_elements = len(lst)\n",
    "    num_groups = num_elements // num_grouped\n",
    "    truncated_lst = lst[:(num_grouped * num_groups)]\n",
    "    return [truncated_lst[i: (i + num_grouped)] \n",
    "            for i in xrange(0, num_elements, num_grouped)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(lst1, lst2):\n",
    "    \"\"\"\n",
    "    Shuffle two lists without changing their correspondences\n",
    "    Args:\n",
    "        lst1: list 1\n",
    "        lst2: list 2\n",
    "    Returns:\n",
    "        The two shuffled lists\n",
    "    \"\"\"\n",
    "    combined = list(zip(lst1, lst2))\n",
    "    np.random.shuffle(combined)\n",
    "    (shuffled_lst1, shuffled_lst2) = zip(*combined)\n",
    "    return [list(shuffled_lst1), list(shuffled_lst2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(lst):\n",
    "    \"\"\"\n",
    "    Remove duplicates from a list\n",
    "    list element can be of any type (including list)\n",
    "    \n",
    "    Caution: the returned list is automatically sorted!\n",
    "    \"\"\"\n",
    "    lst.sort()\n",
    "    lst_without_duplicates = [x for (x, _) in groupby(lst)]\n",
    "    num_removed = len(lst) - len(lst_without_duplicates)\n",
    "    print(\"Removed %d duplicates!\" % num_removed)\n",
    "    return lst_without_duplicates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
