{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from util import load_pickle, dump_pickle, unzip_lst, remove_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_page = -1 # set to -1 to get all pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_lines(filename, lines):\n",
    "    \"\"\"\n",
    "    Write a list to a file line by line \n",
    "    \"\"\"\n",
    "    with open(filename, 'w', encoding=\"utf-8\") as fp:\n",
    "        for line in lines:\n",
    "            print(line, file=fp)\n",
    "    print(\"Done writing to file %s.\" % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_lst(lst, num_grouped):\n",
    "    num_elements = len(lst)\n",
    "    num_groups = num_elements // num_grouped\n",
    "    truncated_lst = lst[:(num_grouped * num_groups)]\n",
    "    return [truncated_lst[i: (i + num_grouped)] \n",
    "            for i in range(0, num_elements, num_grouped)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"words.pkl\"\n",
    "\n",
    "fp = Path(file)\n",
    "if fp.is_file():\n",
    "    pages = load_pickle(file)\n",
    "else:\n",
    "    pdf_file = open('/playpen/home/tongn/Word Frequency List of American English.pdf', 'rb')\n",
    "    read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "    start = 1906\n",
    "    end = 2053\n",
    "    pages = []\n",
    "    for i in range(start, end):\n",
    "        page = read_pdf.getPage(i)\n",
    "        page_content = page.extractText()\n",
    "        pages.append(page_content)\n",
    "    dump_pickle(file, pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entries = []\n",
    "bad_pages = []\n",
    "for (i, page) in enumerate(pages):\n",
    "    if test_page >= 0:\n",
    "        i = test_page\n",
    "        page = pages[test_page]\n",
    "    \n",
    "    if i == 0:\n",
    "        header_len = 50\n",
    "    else:\n",
    "        header_len = 1\n",
    "    page_split = pages[i].split('\\n')[header_len:] # [header_len:] to get ride of page number, etc.\n",
    "    filtered_page = [\n",
    "        line for line in page_split \n",
    "        if (line.strip() != '') and not (len(line) == 1 and 'A' <= line <= 'Z')]\n",
    "    \n",
    "    max_iter = 6\n",
    "    count = 0\n",
    "    while (True):\n",
    "        num_lines = len(filtered_page)\n",
    "        for j in range(num_lines - 1):\n",
    "            if (filtered_page[j].isdigit() and filtered_page[j + 1].isdigit()):\n",
    "#                 print(\"correcting separate digits\", filtered_page[j], filtered_page[j + 1])\n",
    "                filtered_page[j: j + 2] = [filtered_page[j].strip() + filtered_page[j + 1].strip(), '']\n",
    "#                 print(\"corrected:\", filtered_page[j])\n",
    "            try:\n",
    "                if ((not filtered_page[j].isdigit())\n",
    "                    and (not filtered_page[j] == '')\n",
    "                    and (not filtered_page[j + 1].isdigit())\n",
    "                    and (not filtered_page[j + 2].isdigit())):\n",
    "#                     print(\"correcting separate word parts\", filtered_page[j], filtered_page[j + 1])\n",
    "                    filtered_page[j: j + 2] = [filtered_page[j].strip() + filtered_page[j + 1].strip(), '']\n",
    "#                     print(\"corrected:\", filtered_page[j])\n",
    "            except:\n",
    "                pass\n",
    "        filtered_page = [line for line in filtered_page if line != '']\n",
    "        if len(filtered_page) % 3 == 0 and count >= 2: # forced to iterate three times            \n",
    "            print(\"Went through %d iterations\" % (count + 1))\n",
    "            break\n",
    "        count += 1\n",
    "        if count >= max_iter:\n",
    "            print(\"Reached max iterations on page\", i)\n",
    "            bad_pages.append(i)\n",
    "            break\n",
    "\n",
    "    if i == 63: # on page 63, the word \"I\" will be filtered by 'A' <= 'I' <= 'Z'\n",
    "        idx = filtered_page.index(\"11\")\n",
    "        filtered_page.insert(idx + 1, \"I\")\n",
    "        bad_pages.remove(i)\n",
    "    \n",
    "    grouped = group_lst(filtered_page, 3)\n",
    "    try:\n",
    "        sorted(grouped, key=lambda entry: int(entry[0]))\n",
    "    except:\n",
    "        print(\"Can't sort on page\", i)\n",
    "        pprint(grouped)\n",
    "        exit()\n",
    "    entries.extend(grouped)\n",
    "    \n",
    "    if test_page > 0:\n",
    "        pprint(grouped)\n",
    "        break\n",
    "    \n",
    "if bad_pages == []:\n",
    "    sorted_entries = sorted(remove_duplicates(entries), key=lambda entry: int(entry[0]))\n",
    "    str_entries = ['\\t'.join(entry) for entry in sorted_entries]\n",
    "    write_lines(\"word_freq_list.txt\", str_entries)\n",
    "    print(\"Number of words:\", len(str_entries))\n",
    "    print(\"Supposed number of words:\", sorted_entries[-1][0])\n",
    "else:\n",
    "    print(\"Bad pages:\", bad_pages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
